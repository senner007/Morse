{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, Conv2D, Conv1D, MaxPooling2D, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras, config\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPUs Available: \", len(config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_preprocessors import add_zeropad_random, add_noise_random, add_noise\n",
    "# Global settings\n",
    "TEST_SPLIT_SIZE = 0.90\n",
    "VALIDATION_SPLIT_SIZE = 0.90\n",
    "IMAGE_TARGET_SIZE = (5, 1400, 1)\n",
    "BATCH_SIZE =  128\n",
    "LETTER_END_POSITION = \"P1\"\n",
    "IMAGE_PREPOCESSORS = [\n",
    "    # {\"func\" :add_zeropad_random, \"params\": [-10, 15]}, \n",
    "    {\"func\": add_noise_random, \"params\": [0, 30]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from morse_helpers import create_sets\n",
    "from morse_label_funcs import velocity_regression\n",
    "\n",
    "\n",
    "def get_sets():\n",
    "    return [\n",
    "   \n",
    "        # [\"./training_data/MorseTrainSet_17/GEN17_VER_000/\", 'wordsMatrices_17_000', \"Words_17_000.csv\"],\n",
    "        [\"./training_data/MorseTrainSet_17/GEN17_VER_100/\", 'wordsMatrices_17_100', \"Words_17_100.csv\"],\n",
    "        [\"./training_data/MorseTrainSet_17/GEN17_VER_200/\", 'wordsMatrices_17_200', \"Words_17_200.csv\"],\n",
    "        # [\"./training_data/MorseTrainSet_17/GEN17_VER_210/\", 'wordsMatrices_17_210', \"Words_17_210.csv\"],\n",
    "        # [\"./training_data/MorseTrainSet_17/GEN17_VER_220/\", 'wordsMatrices_17_220', \"Words_17_220.csv\"],\n",
    "\n",
    "        [\"./training_data/MorseTrainSet_17/GEN17_VER_001/\", 'wordsMatrices_17_001', \"Words_17_001.csv\"],\n",
    "        [\"./training_data/MorseTrainSet_17/GEN17_VER_002/\", 'wordsMatrices_17_002', \"Words_17_002.csv\"],\n",
    "        [\"./training_data/MorseTrainSet_17/GEN17_VER_011/\", 'wordsMatrices_17_011', \"Words_17_011.csv\"],\n",
    "        [\"./training_data/MorseTrainSet_17/GEN17_VER_012/\", 'wordsMatrices_17_012', \"Words_17_012.csv\"],\n",
    "        # [\"./training_data/MorseTrainSet_17/GEN17_VER_021/\", 'wordsMatrices_17_021', \"Words_17_021.csv\"],\n",
    "        # [\"./training_data/MorseTrainSet_17/GEN17_VER_022/\", 'wordsMatrices_17_022', \"Words_17_022.csv\"],\n",
    "        [\"./training_data/MorseTrainSet_17/GEN17_VER_111/\", 'wordsMatrices_17_111', \"Words_17_111.csv\"],\n",
    "        # [\"./training_data/MorseTrainSet_17/GEN17_VER_222/\", 'wordsMatrices_17_222', \"Words_17_222.csv\"],\n",
    "\n",
    "    ]\n",
    "\n",
    "(image_fnames, morse_labels) = create_sets(\n",
    "    get_sets(), \n",
    "    IMAGE_TARGET_SIZE,\n",
    "    [velocity_regression],\n",
    "    letter_n=LETTER_END_POSITION,\n",
    "    overwrite_images=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morse_labels = np.array(morse_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morse_helpers import create_all_sets\n",
    "\n",
    "train, labels, train_validation, labels_validation, train_test, labels_test = create_all_sets(\n",
    "    image_fnames, morse_labels, TEST_SPLIT_SIZE, VALIDATION_SPLIT_SIZE, shuffle_before_test_split=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morse_helpers import Image_Generator, return_label_positions\n",
    "\n",
    "training_batch_generator = Image_Generator(train, labels, BATCH_SIZE, IMAGE_TARGET_SIZE, IMAGE_PREPOCESSORS, return_label_positions)\n",
    "validation_batch_generator = Image_Generator(train_validation, labels_validation, BATCH_SIZE, IMAGE_TARGET_SIZE, IMAGE_PREPOCESSORS, return_label_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model_velocity(input_layer):\n",
    "    x = keras.layers.Cropping2D(cropping=((0, 0), (0,200)), data_format=None)(input_layer)\n",
    "\n",
    "    x = Conv2D(90,(1,7), padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2,1),padding=\"same\")(x)\n",
    "\n",
    "    x = Conv2D(90,(1,7),padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(1,2),padding=\"same\")(x)\n",
    "\n",
    "    x = Conv2D(90,(1,5),padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(1,2),padding=\"same\")(x)\n",
    "\n",
    "    x = Conv2D(90,(3,3),padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(1,2),padding=\"same\")(x)\n",
    "\n",
    "    x = Conv2D(90,(3,3),padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(1,2),padding=\"same\")(x)\n",
    "\n",
    "    x = Conv2D(90,(3,3),padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(1,2),padding=\"same\")(x)\n",
    "\n",
    "    x = Conv2D(90,(3,3),padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(1,2),padding=\"same\")(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "input_layer    = Input(shape=IMAGE_TARGET_SIZE)\n",
    "conv_model_position_flattened = conv_model_velocity(input_layer)\n",
    "output_layer_position    = Dense(1, name=\"regr\")(conv_model_position_flattened)\n",
    "\n",
    "model           = Model(inputs=input_layer, outputs=output_layer_position)\n",
    "model.compile(loss=[\"mse\"], optimizer='adam', metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "\n",
    "def fit_model(epochs):\n",
    "\t\n",
    "\tglobal init_epoch\n",
    "\thistory = model.fit(\n",
    "\t\t\t\t\t   training_batch_generator,\n",
    "\t                   steps_per_epoch = int(len(train) // BATCH_SIZE),\n",
    "\t                   epochs = epochs + init_epoch,\n",
    "\t\t\t\t\t   initial_epoch=init_epoch,\n",
    "\t                   verbose =1,\n",
    "\t                   validation_data = validation_batch_generator,\n",
    "\t                   validation_steps = int(len(train_validation) // BATCH_SIZE))\n",
    "\t\n",
    "\t\n",
    "\tinit_epoch += epochs\n",
    "\treturn history\n",
    "\n",
    "history = fit_model(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img):\n",
    "    plt.figure(figsize=(30,5))\n",
    "    plt.xlim(0, 300)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo_diff_above = 1\n",
    "\n",
    "def get_deviating_predictions(tempo_diff_above, generator, predictions):\n",
    "    regression_differences = []\n",
    "    indexer = 0\n",
    "\n",
    "    for imgs_batch, labels_batch in generator:\n",
    "\n",
    "        for i in range(len(imgs_batch)):\n",
    "\n",
    "            regr_pred = predictions[indexer] * 100\n",
    "            regr_test_label = labels_batch[i] * 100\n",
    "\n",
    "            if abs(regr_pred[0] - regr_test_label) > tempo_diff_above:\n",
    "                regression_differences.append([regr_pred, regr_test_label, imgs_batch[i]])\n",
    "\n",
    "            indexer += 1\n",
    "\n",
    "    return regression_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : print model summary, model history, image optimizers, training sets used, plot graphs, save to pdf\n",
    "\n",
    "def print_noise_results(predictions, noise_level, tempo_diff_above, differences, evaluations):\n",
    "    print(\"Total predictions:\", len(predictions))\n",
    "    print(\"Noise level:\", noise_level)\n",
    "    print(\"Total velocity predictions off by more than n:\", tempo_diff_above, \":\", len(differences))\n",
    "    print(\"Velocity predictions percentage incorrect:\", round( (len(differences) / len(predictions) * 100), 4), \"%\")\n",
    "    print(\"Model evaluation:\", evaluations)\n",
    "    print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "import inspect\n",
    "\n",
    "# TODO : create json report instead\n",
    "\n",
    "print(\"\\nData sets used:\")\n",
    "print(inspect.getsource(get_sets))\n",
    "\n",
    "print(\"\\nData size:\")\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(train_validation))\n",
    "print(\"Test set size:\", len(train_test))\n",
    "\n",
    "print(\"\\nImage pre-processors used:\")\n",
    "print(IMAGE_PREPOCESSORS)\n",
    "\n",
    "print(\"\\nConvolution layer:\")\n",
    "print(inspect.getsource(conv_model_velocity))\n",
    "\n",
    "print(\"\\nModel summary:\")\n",
    "print(model.summary())\n",
    "print(\"\\nOptimizer:\", model.optimizer)\n",
    "\n",
    "print(\"\\nModel performance:\")\n",
    "print(history.history)\n",
    "print(\"\\nFinal epoch performance:\")\n",
    "for key in history.history.keys():\n",
    "    print(key, history.history[key][-1])\n",
    "\n",
    "print(\"\\nTotal epochs:\", init_epoch)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "\n",
    "noise_levels = [0.2]\n",
    "for noise_level in noise_levels:\n",
    "\n",
    "    image_preprocessors_test = [{\"func\": add_noise, \"params\": noise_level}]\n",
    "\n",
    "    test_batch_generator = Image_Generator(train_test, labels_test, BATCH_SIZE, IMAGE_TARGET_SIZE, image_preprocessors_test, return_label_positions)\n",
    "    predictions = model.predict(test_batch_generator)\n",
    "    regression_differences  = get_deviating_predictions(tempo_diff_above, test_batch_generator, predictions)\n",
    "    evaluations = model.evaluate(test_batch_generator, verbose = 0)\n",
    "\n",
    "    print_noise_results(predictions, noise_level, tempo_diff_above, regression_differences, evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "with open('results_velocity/results_velocity.txt' + timestr, 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counter = 0   \n",
    "for diff in regression_differences:\n",
    "\n",
    "    counter += 1\n",
    "    if counter > 100:\n",
    "        break\n",
    "\n",
    "    pred, correct, img = diff\n",
    "\n",
    "    img_pred = img.copy()\n",
    "    img_correct = img.copy()\n",
    "\n",
    "    print('Prediction', round(pred[0]))\n",
    "    show_image(img_pred)\n",
    "\n",
    "    print('Correct', round(correct))\n",
    "    show_image(img_correct)\n",
    "\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ddce95701716284d8751d5796858f7ca5e76f20d028bc4aeb8f12865d04c55c9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('newenv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
