{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, Conv2D, Conv1D, MaxPooling2D, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras, config\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPUs Available: \", len(config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img):\n",
    "    plt.figure(figsize=(30,5))\n",
    "    plt.xlim(0, 300)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "from image_process.image_preprocessors import add_noise,add_noise_randomly, shift_random_update_positions\n",
    "from data_filters import min_n_letters, take_percent\n",
    "from keras_generators.image_generator import Image_Generator_RAW\n",
    "# Global settings\n",
    "TEST_SPLIT_SIZE = 0.80\n",
    "VALIDATION_SPLIT_SIZE = 0.90\n",
    "IMAGE_TARGET_SIZE = (5, 1400, 1)\n",
    "BATCH_SIZE = 128\n",
    "EPOCH_SIZE = 500\n",
    "LETTER_END_POSITION = \"P1\"\n",
    "ADD_NOISE_RANDOMLY = [0, 30]\n",
    "ADD_SIGNAL_INDENT_RANDOMLY = [0, 1000]\n",
    "IMAGE_PREPOCESSORS = [\n",
    "    {\"func\": add_noise_randomly, \"params\" : ADD_NOISE_RANDOMLY}\n",
    "]\n",
    "MASKS = [\n",
    "    {\"func\": min_n_letters, \"params\" : 3}, \n",
    "    {\"func\": take_percent, \"params\": 100}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# from morse_helpers import create_sets\n",
    "# from morse_label_funcs import velocity_regression_v2\n",
    "\n",
    "# def get_sets():\n",
    "#     return [\n",
    "#         [\"./training_data/MorseTrainSet_18/GEN18_VER_012/\", 'wordsMatrices_18_012', \"Words_18_012.csv\"],\n",
    "#         # [\"./training_data/MorseTrainSet_18/GEN18_VER_021/\", 'wordsMatrices_18_021', \"Words_18_021.csv\"],\n",
    "#         # [\"./training_data/MorseTrainSet_18/GEN18_VER_022/\", 'wordsMatrices_18_022', \"Words_18_022.csv\"],\n",
    "#         # [\"./training_data/MorseTrainSet_18/GEN18_VER_111/\", 'wordsMatrices_18_111', \"Words_18_111.csv\"],\n",
    "#         # [\"./training_data/MorseTrainSet_18/GEN18_VER_222/\", 'wordsMatrices_18_222', \"Words_18_222.csv\"],\n",
    "#         # [\"./training_data/MorseTrainSet_18/GEN18_VER_301/\", 'wordsMatrices_18_301', \"Words_18_301.csv\"],\n",
    "#         # [\"./training_data/MorseTrainSet_18/GEN18_VER_320/\", 'wordsMatrices_18_320', \"Words_18_320.csv\"],\n",
    "#         # [\"./training_data/MorseTrainSet_18/GEN18_VER_411/\", 'wordsMatrices_18_411', \"Words_18_411.csv\"],\n",
    "#         # [\"./training_data/MorseTrainSet_18/GEN18_VER_410/\", 'wordsMatrices_18_410', \"Words_18_410.csv\"],\n",
    "#         # [\"./training_data/MorseTrainSet_18/GEN18_VER_402/\", 'wordsMatrices_18_402', \"Words_18_402.csv\"],\n",
    "#     ]\n",
    "\n",
    "# (image_fnames, morse_labels) = create_sets(\n",
    "#     get_sets(), \n",
    "#     IMAGE_TARGET_SIZE,\n",
    "#     [velocity_regression_v2],\n",
    "#     letter_n=LETTER_END_POSITION,\n",
    "#     overwrite_images=False,\n",
    "#     masks=MASKS\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morse_labels = np.array(morse_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morse_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from morse_helpers import create_all_sets\n",
    "\n",
    "# train, labels, train_validation, labels_validation, train_test, labels_test = create_all_sets(\n",
    "#     image_fnames, morse_labels, TEST_SPLIT_SIZE, VALIDATION_SPLIT_SIZE, shuffle_before_test_split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras_generators.image_generator import Image_Generator\n",
    "# from morse_helpers import return_label_positions\n",
    "\n",
    "# training_batch_generator = Image_Generator(train, labels, BATCH_SIZE, IMAGE_TARGET_SIZE, IMAGE_PREPOCESSORS, return_label_positions)\n",
    "# validation_batch_generator = Image_Generator(train_validation, labels_validation, BATCH_SIZE, IMAGE_TARGET_SIZE, IMAGE_PREPOCESSORS, return_label_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (t,l) = training_batch_generator.__getitem__(0)\n",
    "\n",
    "\n",
    "# for idx,img in enumerate(t):\n",
    "#     print(\"label:\")\n",
    "#     print(l[idx])\n",
    "#     show_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Image_Generator_helpers import  DataSets, set_paths, global_path\n",
    "from data_filters import min_n_letters_raw\n",
    "set_obj = DataSets(set_paths, global_path, [min_n_letters_raw(3)])\n",
    "set_obj.csv_files\n",
    "\n",
    "# (data_frame[\"Tempo\"].values.astype(np.float) / data_frame[\"Tempo Diff\"].values.astype(np.float)) / 100\n",
    "\n",
    "def get_velocity_labels(random_items):\n",
    "    return [(item.csv_row[\"Tempo\"].values.astype(np.float64)[0] / item.csv_row[\"Tempo Diff\"].values.astype(np.float64)[0]) / 100 for item in random_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Train_Generator_RAW = Image_Generator_RAW(\n",
    "    image_amount=BATCH_SIZE * EPOCH_SIZE,\n",
    "    set_obj= set_obj,\n",
    "    FFT_JUMP=64,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_target_size=IMAGE_TARGET_SIZE,\n",
    "    image_prepocessors=\"\",\n",
    "    noise_range=ADD_NOISE_RANDOMLY,\n",
    "    random_signal_indent=ADD_SIGNAL_INDENT_RANDOMLY,\n",
    "    label_func = get_velocity_labels\n",
    "    )\n",
    "\n",
    "# Validation_Generator_RAW = Image_Generator_RAW(\n",
    "#     image_amount=BATCH_SIZE * 1000,\n",
    "#     set_obj= set_obj,\n",
    "#     FFT_JUMP=64,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     image_target_size=IMAGE_TARGET_SIZE,\n",
    "#     image_prepocessors=\"\",\n",
    "#     noise_range=ADD_NOISE_RANDOMLY,\n",
    "#     random_signal_indent=ADD_SIGNAL_INDENT_RANDOMLY,\n",
    "#     label_func = get_velocity_labels\n",
    "#     )\n",
    "\n",
    "# (t,l) = Train_Generator_RAW.__getitem__(0)\n",
    "\n",
    "\n",
    "# for idx,img in enumerate(t):\n",
    "#     print(\"label: \")\n",
    "#     print(l[idx])\n",
    "#     show_image(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model_velocity(input_layer):\n",
    "    x = keras.layers.Cropping2D(cropping=((0, 0), (0,300)), data_format=None)(input_layer)\n",
    "\n",
    "    x = Conv2D(90,(1,7), padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2,1),padding=\"same\")(x)\n",
    "\n",
    "    x = Conv2D(90,(1,7),padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(1,2),padding=\"same\")(x)\n",
    "\n",
    "    x = Conv2D(90,(1,5),padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(1,2),padding=\"same\")(x)\n",
    "\n",
    "    x = Conv2D(90,(3,3),padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(1,2),padding=\"same\")(x)\n",
    "\n",
    "    x = Conv2D(90,(3,3),padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(1,2),padding=\"same\")(x)\n",
    "\n",
    "    x = Conv2D(90,(3,3),padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(1,2),padding=\"same\")(x)\n",
    "\n",
    "    x = Conv2D(90,(3,3),padding=\"same\",activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(1,2),padding=\"same\")(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "input_layer = Input(shape=IMAGE_TARGET_SIZE)\n",
    "conv_model_position_flattened = conv_model_velocity(input_layer)\n",
    "output_layer_position = Dense(1, name=\"regr\")(conv_model_position_flattened)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer_position)\n",
    "model.compile(loss=[\"mse\"], optimizer='adam', metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 5\n",
    "\n",
    "def fit_model(epochs):\n",
    "\t\n",
    "\tglobal init_epoch\n",
    "\thistory = model.fit(\n",
    "\t\tTrain_Generator_RAW,\n",
    "\t\tsteps_per_epoch = EPOCH_SIZE,\n",
    "\t\tepochs = epochs + init_epoch,\n",
    "\t\tinitial_epoch=init_epoch,\n",
    "\t\tverbose =1,\n",
    "\t\t# validation_data = Validation_Generator_RAW,\n",
    "\t\t# validation_steps = 1000,\n",
    "\t\tworkers=12,\n",
    "\t\tuse_multiprocessing=True\n",
    "\t)\n",
    "\t\t\t\t\t\n",
    "\tinit_epoch += epochs\n",
    "\treturn history\n",
    "\n",
    "history = fit_model(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model('saved_model_velocity_raw_23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo_diff_above = 1\n",
    "\n",
    "def sum_up_to(generator):\n",
    "    return generator.__getitem__(0)\n",
    "\n",
    "\n",
    "def get_deviating_predictions(tempo_diff_above, generator):\n",
    "\n",
    "    a_pool = Pool()\n",
    "\n",
    "    result = a_pool.map(sum_up_to, [generator for a in range(generator.__len__())])\n",
    "\n",
    "    regression_differences = []\n",
    "\n",
    "    for imgs_batch, labels_batch in result:\n",
    "\n",
    "        predictions = model.predict_on_batch(imgs_batch) ## make the predictions before the loop, then insert predictions into multiprocessing functions\n",
    "\n",
    "        for i in range(len(imgs_batch)):\n",
    "\n",
    "            regr_pred = predictions[i] * 100\n",
    "            regr_test_label = labels_batch[i] * 100\n",
    "\n",
    "            if abs(regr_pred[0] - regr_test_label) > tempo_diff_above:\n",
    "                regression_differences.append([regr_pred, regr_test_label, imgs_batch[i]])\n",
    "\n",
    "    return regression_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "from training_log import Training_Data_Log, print_name, json_to_file\n",
    "import inspect\n",
    "\n",
    "data_log = Training_Data_Log()\n",
    "data_log.model_config = model.to_json()\n",
    "data_log.model_config_method_string = [inspect.getsource(conv_model_velocity)]\n",
    "# data_log.training_set_size = len(train)\n",
    "# data_log.validation_set_size = len(train_validation)\n",
    "# data_log.test_set_size = len(train_test)\n",
    "data_log.image_pre_processors = print_name(IMAGE_PREPOCESSORS)\n",
    "data_log.noise_added = ADD_NOISE_RANDOMLY\n",
    "data_log.training_data_masks = print_name(MASKS)\n",
    "data_log.model_summary = model.summary()\n",
    "data_log.model_optimizer = str(type(model.optimizer))\n",
    "data_log.model_history = history.history\n",
    "data_log.model_history_final_epoch = {k: v[-1] for k, v in history.history.items()}\n",
    "data_log.total_epochs = init_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Velocity_Results:\n",
    "    total_predictions = None\n",
    "    noise_level = None\n",
    "    difference = None\n",
    "    image_preprocessors_test = None\n",
    "    predictions_off_by_more_than_difference = None\n",
    "    predictions_incorrect_prercent = None\n",
    "    model_evaluation = None\n",
    "\n",
    "noise_levels = [0.0]\n",
    "result_array = []\n",
    "for noise_level in noise_levels:\n",
    "\n",
    "    batches = 50\n",
    "\n",
    "    # image_preprocessors_test = [{\"func\": add_noise, \"params\" : noise_level}]\n",
    "    test_batch_generator = Train_Generator_RAW = Image_Generator_RAW(\n",
    "        image_amount=BATCH_SIZE * batches,\n",
    "        set_obj= set_obj,\n",
    "        FFT_JUMP=64,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_target_size=IMAGE_TARGET_SIZE,\n",
    "        image_prepocessors=\"\",\n",
    "        noise_range=ADD_NOISE_RANDOMLY,\n",
    "        random_signal_indent=ADD_SIGNAL_INDENT_RANDOMLY,\n",
    "        label_func = get_velocity_labels\n",
    "    )\n",
    "    # predictions = model.predict(test_batch_generator)\n",
    "    # model.predict_on_batch\n",
    "\n",
    "    velocity_differences  = get_deviating_predictions(tempo_diff_above, test_batch_generator)\n",
    "    evaluations = model.evaluate(test_batch_generator, verbose = 0)\n",
    "\n",
    "    results = Velocity_Results()\n",
    "    results.total_predictions = batches * BATCH_SIZE\n",
    "    results.noise_level = noise_level\n",
    "    results.difference = tempo_diff_above\n",
    "    # results.image_preprocessors_test = print_name(image_preprocessors_test)\n",
    "    results.predictions_off_by_more_than_difference = len(velocity_differences)\n",
    "    results.predictions_incorrect_prercent = round( (  len(velocity_differences) / (batches * BATCH_SIZE)  ) * 100, 4)\n",
    "    results.model_evaluation = evaluations\n",
    "\n",
    "    result_array.append(results.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log.results = result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_file(\"logs/velocity/velocity_data_log\", data_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, diff in enumerate(velocity_differences):\n",
    "\n",
    "    if idx > 10:\n",
    "        break\n",
    "\n",
    "    pred, correct, img = diff\n",
    "\n",
    "    print('Prediction', round(pred[0]))\n",
    "    print('Correct', round(correct))\n",
    "    show_image(img)\n",
    "\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model_velocity_raw_23_cop_width_300')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ddce95701716284d8751d5796858f7ca5e76f20d028bc4aeb8f12865d04c55c9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('newenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
